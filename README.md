# RuATD22 Competition (binary task)

# Структура проекта

* bert/  - каталог со скриптами для дообучения модели BERT;
* others/ - каталог со скриптами извлечения признаков из рускких текстов;
* models/ - сохраненные модели (не BERT);
* dataset/ - файлы исходного и обработанного датасета (доступны по ссылке [Google Disk](https://drive.google.com/file/d/10OC2UdGLJXJ7Jf6QjrvxI6p_ujQU8UUC/view?usp=sharing));
* ru_atd.ipynb - jupyter notebook с экспериментами.

# Эксперименты

В рамках соревнования были проведены следующие эксперименты:

1. Извлечение признаков из текстов согласно статье ["GLTR: Statistical Detection and Visualization of Generated Text"](https://arxiv.org/abs/1906.04043).
2. Извлечение признаков из текстов согласно статье ["Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover"](https://pubmed.ncbi.nlm.nih.gov/33954234/).
3. Дообучение BERT. В качестве реализации использовался DeepPavlov/rubert-base-cased.
4. Ансамбль классификаторов из экспериментов №2 и №3.
5. Комбинация признаков из экспериментов №2 и №3.

## Эксперимент №1. Извлечение признаков из текстов GLTR

Извлечены признаки согласно статье ["GLTR: Statistical Detection and Visualization of Generated Text"](https://arxiv.org/abs/1906.04043).

1. Пропорция использования первых 10-и самых вероятных в тексте.
2. Пропорция использования 100-и первых самых вероятных слов в тексте, кроме п.1.
3. Пропорция использования 1000-и первых самых вероятных слов в тексте, кроме п.1 и п.2.
4. Пропорция использования маловероятных слов, которые идут после 1000-и первых самых вероятных слов в тексте.
5. Медиана долей вероятностей токенов.
6. Медиана долей энтропии.
7. Длинна текста в токенах.

Обучение выполнялось на 10000 примеров из обучающей выборки для каждого класса, из них 10% отобрано для валидационной выборки. Для обучения на всей выборке можно доработать извлечение признаков в части пакетного предсказания распределений.

Проблемы: в текущей реализации предсказание делается для одного примера за раз (без батчинга), что существенно увеличивает время на стадии извлечения признаков.

Результаты: качество на 10000 примеров оказалось низким (менее 60% accuracy для логистической регрессии), было принято решение переключиться на другие гипотезы.

## Эксперимент №2. Извлечение признаков из текстов

Извлечены признаки согласно статье ["Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover"](https://pubmed.ncbi.nlm.nih.gov/33954234/), кроме следующих признаков, для которых не были найдены готовые решения для русского языка:

1. Признаки кореференции.
2. Признаки эмпатии.

Рассматриваемые классификаторы:

1. Logistic Regression.
2. Histogram-based Gradient Boosting Classification Tree.

## Эксперимент №3. Дообучение BERT

Дообучение выполнялось в двух режимах:

1. С ранней остановкой, если ошибка на валидационной выборке не уменьшается после эпохи.
2. С ранней остановкой, если метрика доли верных ответов (accuracy) не улучшилась на валидацонной выборке после N шагов обучения. Показан лучший результат на тестовой выборке по accuracy.

## Эксперимент №4. Ансамбль классификаторов

Ансамбль включал следующие классификаторы:

* BERT из эксперимента №3;
* Logistic Regression;
* Histogram-based Gradient Boosting Classification Tree.

Для построения ансамбля использовался классификатор VotingClassifier в режиме soft голосования.

## Эксперимент №5. Комбинация признаков

Комбинация признаков включает в свой состав:

* Скрытое состояние BERT на последнем слое;
* Признаки из эксперимента №2.

В качестве классификатора использовался Logistic Regression.

# Результаты

В таблице приведены оценки точности, полученные в рамках экспериментов на тестовой выборке, если не указано другое в примечании.

|               Эксперимент                | Accuracy |                             Примечание                              |
| ---------------------------------------- | -------- | ------------------------------------------------------------------- |
| №1. Извлечение признаков из текстов GLTR | 0.56     | Точность на валидационной выборке, после обучения на 10000 примеров |
| №2. Извлечение признаков из текстов      | 0.68607  | Классификатор HistGradientBoosting                                  |
| №3. Дообучение BERT                      | 0.81138  | Лучшие результат с остановкой после N шагов обучения                |
| №4. Ансамбль классификаторов             | 0.81060  |                                                                     |
| №5. Комбинация признаков                 | 0.76551  |                                                                     |

# Возможные улучшения

* Выполнить извлечение признаков в эксперименте №1 (GLTR) для всех примеров обучающей выборки;
* Использовать большие языковые модели для извлечение признаков в эксперимента №1 (GLTR);
* Выполнить извлечение дополнительных признаков для эксперимента №2 (например, признаки кореференции, эмпатии).
